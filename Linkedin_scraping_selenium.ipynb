{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd22d40",
   "metadata": {},
   "source": [
    "# Import All Necessary Libraries\n",
    "\n",
    "The objective of the code below is to scrap data on Data Science jobs on LinkedIn located in Barcelona.\n",
    "The libraries used are the following:\n",
    "<ul>\n",
    "  <li>numpy</li>\n",
    "  <li>pandas</li>\n",
    "  <li>bs4</li>\n",
    "  <li>time</li>\n",
    "  <li>selenium</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa3c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariehartmann/opt/anaconda3/lib/python3.9/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d5b3c",
   "metadata": {},
   "source": [
    "# ChromeDriver\n",
    "\n",
    "ChromeDriver is used to nagivate the browser, it can be downlaoded from this <a href=\"https://chromedriver.storage.googleapis.com/index.html?path=107.0.5304.62/\">link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1813c907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/s24tgs111bx3dwpv_gd4m48r0000gn/T/ipykernel_82560/4178745537.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=driver_path)\n"
     ]
    }
   ],
   "source": [
    "driver_path = \"chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=driver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f47519",
   "metadata": {},
   "source": [
    "# Opening LinkedIn\n",
    "\n",
    "The below code will autoamtically open LinkedIn and execute the following:\n",
    "<ul>\n",
    "  <li>Wait to ensure the page is fully loaded</li>\n",
    "  <li>Calcualte the screen height to be used for scrolling one page at a time</li>\n",
    "  <li>Pause in between screen heights to ensure the page is loaded to accurately find necessary elements</li>\n",
    "</ul>\n",
    "\n",
    "## Creating Data Frame of Jobs\n",
    "\n",
    "Now that the page is fully loaded to the planned capability, we begin scrapping the data. The below code uses both HTML and Python and follows the below steps:\n",
    "\n",
    "\n",
    " <ul>\n",
    "  <li>Parse to HTML</li>\n",
    "  <li>Find unordered list with jobs</li>\n",
    "  <li>Find all elements in the list</li>\n",
    "  <li>Take the url of the detailed page for one job, request and parse to html </li>\n",
    "  <li>Find the title, company, and location from the detailed page</li>\n",
    "  <li>Look for the current state of the job and categorize into: \"Early Application\", \"On-going Application\", and \"Other\"</li>\n",
    "  <li>Find date since job was published</li>\n",
    "  <li>Print all the jobs</li>\n",
    "  <li>Retrieve number of applicants, a try and escept block is used because the tag and class differs for applicants that are lower than 25 or higher than 200.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb95e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening linkedIn's login page\n",
    "driver.get(\"https://www.linkedin.com/jobs/search/?currentJobId=3270287326&geoId=107025191&keywords=data%20scientist%2C%20barcelona&location=Barcelona%2C%20Catalonia%2C%20Spain&refresh=true&position=1&pageNum=0\")\n",
    " \n",
    "# waiting for the page to load\n",
    "time.sleep(1)\n",
    "\n",
    "scroll_pause_time = 1\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "i = 0\n",
    "\n",
    "while True and i < 190: # set limit to stop when no more \"load more results\" available; # of scrolls can vary so include a buffer\n",
    "    #scroll one screen height every time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))\n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "    if screen_height * i > scroll_height:\n",
    "        try:\n",
    "            element=driver.find_element(By.CLASS_NAME, \"infinite-scroller__show-more-button\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            driver.execute_script(\"arguments[0].click();\", element)      \n",
    "        except:\n",
    "            time.sleep(scroll_pause_time)\n",
    "            driver.find_element(By.CSS_SELECTOR, \"button[class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                  \n",
    "j = 1\n",
    "soup = bs4.BeautifulSoup(driver.page_source, \"html.parser\") #parse to HTML\n",
    "resList = soup.find(\"ul\", {\"class\": \"jobs-search__results-list\"}) #find unordered list with jobs\n",
    "jobs = resList.find_all(\"li\") #find all the elements in the list\n",
    "#print(len(jobs))\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Job Title\", \"Company Name\", \"Location\", \"State\", \"Posting Date\", \"Offer URL\", \"Number of Applicants\",\n",
    "                            \"Promoted\", \"Workspace\", \"Seniority\", \"Employment Type\", \"Industry\", \"Python Required\",\n",
    "                            \"Application through Linkedin\", \"Number of Employees\"])\n",
    "\n",
    "for job in jobs: #loop on all the jobs in the list\n",
    "    \n",
    "    url = job.a[\"href\"] #take the url of the detailed page for one job\n",
    "    detailedPage = requests.get(url) #request to that url\n",
    "    detailedSoup = bs4.BeautifulSoup(detailedPage.text, \"html.parser\") #parse to html\n",
    "    \n",
    "    #find title, company and location from the detailed page; if not present, don't add the job\n",
    "    generalInfo = detailedSoup.find(\"div\", {\"class\": \"top-card-layout__card relative p-2 papabear:p-details-container-padding\"}) #find the section with hte main info\n",
    "    try:\n",
    "        title = generalInfo.find(\"h1\").text.strip() #take the title and remove the extra spaces with strip\n",
    "        company = generalInfo.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.split(\"\\n\")[1].strip()\n",
    "        location = generalInfo.find(\"span\", {\"class\": \"topcard__flavor topcard__flavor--bullet\"}).text.split(\"\\n\")[1].strip()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    #Find the state from the general page\n",
    "    stateText = job.find(\"span\", {\"class\": \"result-benefits__text\"})\n",
    "    if stateText is None: #if the alert is not there, state is None\n",
    "        state = np.NaN\n",
    "    else: #if the alert exists, we can take the text, strip and find the right state according to the situation\n",
    "        stateText = stateText.text.strip()\n",
    "        if stateText == \"Be an early applicant\":\n",
    "            state = \"Early Applications\"\n",
    "        elif stateText == \"Actively Hiring\":\n",
    "            state = \"On-going\"\n",
    "        else:\n",
    "            state = \"Others\"\n",
    "        \n",
    "    \n",
    "    #date since publication of the job\n",
    "    date = job.find(\"time\", {\"class\": \"job-search-card__listdate\"})\n",
    "    if date is None:\n",
    "        date = job.find(\"time\", {\"class\": \"job-search-card__listdate--new\"})\n",
    "    date = date.text.strip()\n",
    "    \n",
    "    #retrieve the number of applicants; try-except block because when number of applicants is lower than 25 or higher than 200,\n",
    "    # the tag and the class are different\n",
    "    try:\n",
    "        applicants = generalInfo.find(\"span\", {\"class\": \"num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        applicants = generalInfo.find(\"figcaption\", {\"class\": \"num-applicants__caption\"}).text.strip()\n",
    "        \n",
    "    applicants = applicants.split(\" \") #split at the space\n",
    "    if len(applicants) == 6: #this is the case when <25 applicants: \"SÃ© de los primeros 25 solicitantes\"\n",
    "        applicants = int(applicants[4])\n",
    "    elif len(applicants) == 4: #this is the case when >200 applicants: \"Mas de 200 solicitudes\"\n",
    "        applicants = int(applicants[2])\n",
    "    else:\n",
    "        applicants = int(applicants[0]) #this is the regular case: \"x solicitudes\"\n",
    "    \n",
    "   \n",
    "    \n",
    "    coreSection = detailedSoup.find(\"div\", {\"class\": \"core-section-container__content break-words\"}) #find the section in the middle of the page\n",
    "    items = coreSection.find_all(\"span\", {\"class\": \"description__job-criteria-text description__job-criteria-text--criteria\"}) #find the 4 items there\n",
    "    if len(items) == 4: #if they are all present\n",
    "        seniority = items[0].text.strip()\n",
    "        employment = items[1].text.strip()\n",
    "        industry = items[3].text.strip()\n",
    "    else: #if only the employment is present (it happens in some cases)\n",
    "        employment = items[0].text.strip()\n",
    "        seniority = np.NaN\n",
    "        industry = np.NaN\n",
    "    \n",
    "    #Python required\n",
    "    description = detailedSoup.find(\"div\", {\"class\": \"show-more-less-html__markup\"}).text #find the description section and take text\n",
    "    test_string = [\"Python\", \"python\", \"PYTHON\"] #define list with possible words to look for (not case sensitive)\n",
    "    if any(word in description for word in test_string): #if the word python written in any way is there\n",
    "        required = True #then required is true\n",
    "    else:\n",
    "        required = False #otherwise is false\n",
    "        \n",
    "    #Application through Linkedin\n",
    "    applyFromCompany = detailedSoup.find(\"a\", {\"class\": \"apply-button apply-button--link top-card-layout__cta mt-2 ml-1.5 h-auto babybear:flex-auto top-card-layout__cta--primary btn-md btn-primary\"})\n",
    "    if applyFromCompany is not None:\n",
    "        easyApply = False\n",
    "    else:\n",
    "        applyFromLinkedin = detailedSoup.find(\"button\", {\"class\": \"apply-button apply-button--default top-card-layout__cta mt-2 ml-1.5 h-auto babybear:flex-auto top-card-layout__cta--primary btn-md btn-primary\"})\n",
    "        if applyFromLinkedin is not None:\n",
    "            easyApply = True\n",
    "        else:\n",
    "            easyApply = np.NaN\n",
    "    \n",
    "    \n",
    "    my_dict = {\"Job Title\": title,\n",
    "              \"Company Name\": company,\n",
    "              \"Location\": location,\n",
    "              \"State\": state,\n",
    "              \"Posting Date\": date,\n",
    "              \"Offer URL\": url,\n",
    "              \"Number of Applicants\": applicants,\n",
    "              \"Promoted\": np.NaN,\n",
    "              \"Workspace\": np.NaN,\n",
    "              \"Seniority\": seniority,\n",
    "              \"Employment Type\": employment,\n",
    "              \"Industry\": industry,\n",
    "              \"Python Required\": required,\n",
    "              \"Application through Linkedin\": easyApply,\n",
    "              \"Number of Employees\": np.NaN}\n",
    "    \n",
    "    entry = pd.DataFrame([my_dict])\n",
    "    df = pd.concat([df, entry], ignore_index = True)\n",
    "    \n",
    "    \n",
    "    print(j, \"jobs added to the df\")\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"DATAFRAME FILLED\") #note that few instances might not have been added because of blocks from Linkedin (so rows almost 1000)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b437c",
   "metadata": {},
   "source": [
    "# Notes on Un-Retrievable Data\n",
    "\n",
    "The below points could not be retrieved, however the code that would be used if it was retrievable is under each case.\n",
    "\n",
    "### Promoted\n",
    "Information on whether or not a job has been promoted could not be retrieved, if it could have been the below code would be used: \n",
    "\n",
    "`promotedSection = job.find(\"li\", {\"class\": \"t-12 t-normal t-black--light job-card-container__footer-item\"})\n",
    "    if promotedSection is not None:\n",
    "        promoted = True\n",
    "    else:\n",
    "        promoted = False`\n",
    "    \n",
    "### Workplace\n",
    "Information on the workplace type of a job could not be retrieved, if it could we would use the below code:\n",
    "\n",
    "`workspace - NOT POSSIBLE - The following would be the code to retrieve the info if we could\n",
    "            workspace = detailedSoup.find(\"span\", {\"class\": \"jobs-unified-top-card__workplace-type\"})`\n",
    "            \n",
    "### Number of Employees\n",
    "The following would be the code to retrieve the info if we could\n",
    "\n",
    "`companyNameUrl = generalInfo.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"})[\"href\"]\n",
    "companyDetail = requests.get(companyNameUrl)\n",
    "companyDetailSoup = bs4.BeautifulSoup(companyDetail.text, \"html.parser\")\n",
    "employees = companyDetailSoup.find(\"a\", {\"class\": \"face-pile__cta self-center link-no-visited-state\"})`\n",
    "\n",
    "### Company Name URL\n",
    "The following would be the code to retrieve the company name url:\n",
    "\n",
    "`companyNameUrl = generalInfo.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"})[\"href\"]\n",
    "companyDetail = requests.get(companyNameUrl)\n",
    "companyDetailSoup = bs4.BeautifulSoup(companyDetail.text, \"html.parser\")\n",
    "employees = companyDetailSoup.find(\"a\", {\"class\": \"face-pile__cta self-center link-no-visited-state\"})`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094be18b",
   "metadata": {},
   "source": [
    "# Print Data Frame\n",
    "\n",
    "This will gather all the data gathered intot he dataframe, note that some instances might not have been added because of blocks from LinkedIn. Rows printed below should be almost 1,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54808bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATAFRAME FILLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23d784",
   "metadata": {},
   "source": [
    "# Display and Save Data Frame\n",
    "\n",
    "Show the beginning and thend of the dataframe, and finally save the dataframe into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3737ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.to_csv(\"linkedin.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
